{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ME7: Neural Networks\n",
    "\n",
    "We will have serveral exercises of neural networks using Sklearn neural network multi-layer perceptron (MLP) packages. Although keras with tensorflow framework is more commonly used in ML, this will be a good start to know how NN can be used for classification and regression problems. Keras/tensorflow will be explored in the next assignment. \n",
    "\n",
    "Please read the document on Scikit Learn:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your name and your collaborators if any. \n",
    "\n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "\n",
    "cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"NN\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_regions_for_classifier_subplot(clf, X, y, X_test, y_test, title, subplot, target_names = None, plot_decision_regions = True):\n",
    "\n",
    "    numClasses = np.amax(y) + 1\n",
    "    color_list_light = ['#FFFFAA', '#EFEFEF', '#AAFFAA', '#AAAAFF']\n",
    "    color_list_bold = ['#EEEE00', '#000000', '#00CC00', '#0000CC']\n",
    "    cmap_light = ListedColormap(color_list_light[0:numClasses])\n",
    "    cmap_bold  = ListedColormap(color_list_bold[0:numClasses])\n",
    "\n",
    "    h = 0.03\n",
    "    k = 0.5\n",
    "    x_plot_adjust = 0.1\n",
    "    y_plot_adjust = 0.1\n",
    "    plot_symbol_size = 50\n",
    "\n",
    "    x_min = X[:, 0].min()\n",
    "    x_max = X[:, 0].max()\n",
    "    y_min = X[:, 1].min()\n",
    "    y_max = X[:, 1].max()\n",
    "    x2, y2 = np.meshgrid(np.arange(x_min-k, x_max+k, h), np.arange(y_min-k, y_max+k, h))\n",
    "\n",
    "    P = clf.predict(np.c_[x2.ravel(), y2.ravel()])\n",
    "    P = P.reshape(x2.shape)\n",
    "\n",
    "    if plot_decision_regions:\n",
    "        subplot.contourf(x2, y2, P, cmap=cmap_light, alpha = 0.8)\n",
    "\n",
    "    subplot.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=plot_symbol_size, edgecolor = 'black')\n",
    "    subplot.set_xlim(x_min - x_plot_adjust, x_max + x_plot_adjust)\n",
    "    subplot.set_ylim(y_min - y_plot_adjust, y_max + y_plot_adjust)\n",
    "\n",
    "    if (X_test is not None):\n",
    "        subplot.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, s=plot_symbol_size, marker='^', edgecolor = 'black')\n",
    "        train_score = clf.score(X, y)\n",
    "        test_score  = clf.score(X_test, y_test)\n",
    "        title = title + \"\\nTrain score = {:.2f}, Test score = {:.2f}\".format(train_score, test_score)\n",
    "\n",
    "    subplot.set_title(title)\n",
    "\n",
    "    if (target_names is not None):\n",
    "        legend_handles = []\n",
    "        for i in range(0, len(target_names)):\n",
    "            patch = mpatches.Patch(color=color_list_bold[i], label=target_names[i])\n",
    "            legend_handles.append(patch)\n",
    "        subplot.legend(loc=0, handles=legend_handles)\n",
    "\n",
    "\n",
    "def plot_class_regions_for_classifier(clf, X, y, X_test=None, y_test=None, title=None, target_names = None, plot_decision_regions = True):\n",
    "\n",
    "    numClasses = np.amax(y) + 1\n",
    "    color_list_light = ['#FFFFAA', '#EFEFEF', '#AAFFAA', '#AAAAFF']\n",
    "    color_list_bold = ['#EEEE00', '#000000', '#00CC00', '#0000CC']\n",
    "    cmap_light = ListedColormap(color_list_light[0:numClasses])\n",
    "    cmap_bold  = ListedColormap(color_list_bold[0:numClasses])\n",
    "\n",
    "    h = 0.03\n",
    "    k = 0.5\n",
    "    x_plot_adjust = 0.1\n",
    "    y_plot_adjust = 0.1\n",
    "    plot_symbol_size = 50\n",
    "\n",
    "    x_min = X[:, 0].min()\n",
    "    x_max = X[:, 0].max()\n",
    "    y_min = X[:, 1].min()\n",
    "    y_max = X[:, 1].max()\n",
    "    x2, y2 = np.meshgrid(np.arange(x_min-k, x_max+k, h), np.arange(y_min-k, y_max+k, h))\n",
    "\n",
    "    P = clf.predict(np.c_[x2.ravel(), y2.ravel()])\n",
    "    P = P.reshape(x2.shape)\n",
    "    plt.figure()\n",
    "    if plot_decision_regions:\n",
    "        plt.contourf(x2, y2, P, cmap=cmap_light, alpha = 0.8)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=plot_symbol_size, edgecolor = 'black')\n",
    "    plt.xlim(x_min - x_plot_adjust, x_max + x_plot_adjust)\n",
    "    plt.ylim(y_min - y_plot_adjust, y_max + y_plot_adjust)\n",
    "\n",
    "    if (X_test is not None):\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, s=plot_symbol_size, marker='^', edgecolor = 'black')\n",
    "        train_score = clf.score(X, y)\n",
    "        test_score  = clf.score(X_test, y_test)\n",
    "        title = title + \"\\nTrain score = {:.2f}, Test score = {:.2f}\".format(train_score, test_score)\n",
    "\n",
    "    if (target_names is not None):\n",
    "        legend_handles = []\n",
    "        for i in range(0, len(target_names)):\n",
    "            patch = mpatches.Patch(color=color_list_bold[i], label=target_names[i])\n",
    "            legend_handles.append(patch)\n",
    "        plt.legend(loc=0, handles=legend_handles)\n",
    "\n",
    "    if (title is not None):\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "- You should explore these activation functions to find the best function for your dataset(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = np.linspace(-2, 2, 200)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "\n",
    "plt.plot(xrange, np.maximum(xrange, 0), label = 'relu')\n",
    "plt.plot(xrange, np.tanh(xrange), label = 'tanh')\n",
    "plt.plot(xrange, 1 / (1 + np.exp(-xrange)), label = 'logistic')\n",
    "plt.legend()\n",
    "plt.title('Neural network activation functions')\n",
    "plt.xlabel('Input value (x)')\n",
    "plt.ylabel('Activation function output')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0\n",
    "\n",
    "- The examples in Part 0 build classifiers and regressors using neural networks. \n",
    "\n",
    "- We will use synthetic datasets to demonstrate the NN modeling process.\n",
    "\n",
    "- Classifier examples demonstrate the model performance through the class boundary with model score (overall accuracy). \n",
    "\n",
    "\n",
    "### Neural networks on Classification\n",
    "- Read and run each cell of the given examples and understand the results\n",
    "\n",
    "- Tasks: you might have warnings related to data normalization or/and number of iterations. \n",
    "    - <span style=\"color:red\"> Fix the issues and remove warnings (if possible).  </span>\n",
    "     \n",
    "\n",
    "#### SkLearn Neural networks for classification\n",
    "- Please also read the document on Scikit Learn   \n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Synthetic dataset1: Binary classification\n",
    "\n",
    "- A synthetic dataset contains two features (x1 and x2)\n",
    "- We use single hidden layer NN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic dataset for classification (binary)\n",
    "plt.figure()\n",
    "plt.title('Sample binary classification problem with two informative features')\n",
    "X_C2, y_C2 = make_classification(n_samples = 100, n_features=2,\n",
    "                                n_redundant=0, n_informative=2,\n",
    "                                n_clusters_per_class=1, flip_y = 0.1,\n",
    "                                class_sep = 0.5, random_state=0)\n",
    "\n",
    "plt.scatter(X_C2[:, 0], X_C2[:, 1], marker= 'o', c=y_C2, s=50, cmap=cmap_bold)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN modeling on synthetic dataset 1\n",
    "\n",
    "- We may need to increase the number of iterations or scale the data (normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "fig, subaxes = plt.subplots(3, 1, figsize=(6,18))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state=0)\n",
    "\n",
    "\n",
    "for units, axis in zip([1, 10, 100], subaxes):\n",
    "    # create a model and training it\n",
    "    # we may need to increase the number of iterations or scale the data (normalization)\n",
    "    nnclf = MLPClassifier(hidden_layer_sizes = [units], solver='lbfgs',\n",
    "                         random_state = 0).fit(X_train, y_train)\n",
    "    \n",
    "    title = 'Dataset 1: Neural net classifier, 1 layer, {} units'.format(units)\n",
    "    \n",
    "    plot_class_regions_for_classifier_subplot(nnclf, X_train, y_train,\n",
    "                                             X_test, y_test, title, axis)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Synthetic dataset 2: binary classification\n",
    "\n",
    "- More difficult synthetic dataset for classification (binary) with classes that are not linearly separable.\n",
    "- We apply single hidden layer NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate a synthetic dataset\n",
    "X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2,\n",
    "                       centers = 8, cluster_std = 1.3,\n",
    "                       random_state = 4)\n",
    "y_D2 = y_D2 % 2\n",
    "plt.figure()\n",
    "plt.title('Sample binary classification problem with non-linearly separable classes')\n",
    "plt.scatter(X_D2[:,0], X_D2[:,1], c=y_D2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN modeling on synthetic dataset 2\n",
    "\n",
    "- We may need to increase the number of iterations or scale the data (normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "fig, subaxes = plt.subplots(3, 1, figsize=(6,18))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "\n",
    "for units, axis in zip([1, 10, 100], subaxes):\n",
    "    # create a model and training it\n",
    "    # we may need to increase the number of iterations or scale the data (normalization)\n",
    "    nnclf = MLPClassifier(hidden_layer_sizes = [units], solver='lbfgs',\n",
    "                         random_state = 0).fit(X_train, y_train)\n",
    "    \n",
    "    title = 'Dataset 1: Neural net classifier, 1 layer, {} units'.format(units)\n",
    "    \n",
    "    plot_class_regions_for_classifier_subplot(nnclf, X_train, y_train,\n",
    "                                             X_test, y_test, title, axis)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply two hidden layer NN on synthetic dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from adspy_shared_utilities import plot_class_regions_for_classifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "# model training with two hidden layers\n",
    "nnclf = MLPClassifier(hidden_layer_sizes = [10, 10], solver='lbfgs',\n",
    "                     random_state = 0).fit(X_train, y_train)\n",
    "\n",
    "plot_class_regions_for_classifier(nnclf, X_train, y_train, X_test, y_test,\n",
    "                                 'Dataset 1: Neural net classifier, 2 layers, 10/10 units')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN on synthetic dataset 2: Regularization parameter: alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "fig, subaxes = plt.subplots(4, 1, figsize=(6, 23))\n",
    "\n",
    "for this_alpha, axis in zip([0.01, 0.1, 1.0, 5.0], subaxes):\n",
    "    nnclf = MLPClassifier(solver='lbfgs', activation = 'tanh',\n",
    "                         alpha = this_alpha,\n",
    "                         hidden_layer_sizes = [100, 100],\n",
    "                         random_state = 0).fit(X_train, y_train)\n",
    "    \n",
    "    title = 'Dataset 2: NN classifier, alpha = {:.3f} '.format(this_alpha)\n",
    "    \n",
    "    plot_class_regions_for_classifier_subplot(nnclf, X_train, y_train,\n",
    "                                             X_test, y_test, title, axis)\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN on synthetic dataset2: the effect of different choices of activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "fig, subaxes = plt.subplots(3, 1, figsize=(6,18))\n",
    "\n",
    "for this_activation, axis in zip(['logistic', 'tanh', 'relu'], subaxes):\n",
    "    nnclf = MLPClassifier(max_iter = 500, solver='lbfgs', activation = this_activation,\n",
    "                         alpha = 0.01, hidden_layer_sizes = [100, 100],\n",
    "                         random_state = 0).fit(X_train, y_train)\n",
    "    \n",
    "    title = 'Dataset 2: NN classifier, 2 layers 10/10, {} \\\n",
    "activation function'.format(this_activation)\n",
    "    \n",
    "    plot_class_regions_for_classifier_subplot(nnclf, X_train, y_train,\n",
    "                                             X_test, y_test, title, axis)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks on Regression\n",
    "\n",
    "- NN can be also applied for regression problems.\n",
    "\n",
    "#### sklearn Neural Networks for regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A synthetic dataset with one feature\n",
    "\n",
    "- A simple example that can be visualized the regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic dataset for simple regression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Sample regression problem with one input variable')\n",
    "X_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n",
    "                            n_informative=1, bias = 150.0,\n",
    "                            noise = 30, random_state=0)\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A NN regressor modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "fig, subaxes = plt.subplots(2, 3, figsize=(11,8), dpi=70)\n",
    "\n",
    "X_predict_input = np.linspace(-3, 3, 50).reshape(-1,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1[0::5], y_R1[0::5], random_state = 0)\n",
    "\n",
    "for thisaxisrow, thisactivation in zip(subaxes, ['tanh', 'relu', 'logistic']):\n",
    "    for thisalpha, thisaxis in zip([0.0001, 0.1, 1.0, 100], thisaxisrow):\n",
    "        # create a model\n",
    "        mlpreg = MLPRegressor(hidden_layer_sizes = [100,100],\n",
    "                             activation = thisactivation,\n",
    "                             alpha = thisalpha,\n",
    "                             solver = 'lbfgs').fit(X_train, y_train)\n",
    "        \n",
    "        y_predict_output = mlpreg.predict(X_predict_input)\n",
    "        thisaxis.set_xlim([-2.5, 0.75])\n",
    "        thisaxis.plot(X_predict_input, y_predict_output,\n",
    "                     '^', markersize = 10)\n",
    "        thisaxis.plot(X_train, y_train, 'o')\n",
    "        thisaxis.set_xlabel('Input feature')\n",
    "        thisaxis.set_ylabel('Target value')\n",
    "        thisaxis.set_title('MLP regression\\nalpha={}, activation={})'\n",
    "                          .format(thisalpha, thisactivation))\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "### NN Application to real-world dataset for classification\n",
    "\n",
    "- You will be working on two datasets: (1) Breast cancer dataset, and (2) fruit dataset\n",
    "\n",
    "- Before you start NN modeling, prepare the datasets (a) a dataset without normalization, (b) a dataset with normalization. \n",
    "\n",
    "- Please make sure that training data and test data must be the same scale. This means that the normalization process should be done before you split the data into training data and test data.  \n",
    "\n",
    "- For each dataset, conduct classificaton modeling and evalute the model performance. We would suggest you use evaluation metrics for classification (accuracy, precision, recall, f1-score, etc.)\n",
    "\n",
    "    - 1. Apply neural network with 2 hidden layers with varying number of units (e.g., 10, 20, 50, 100) for each layer.\n",
    "       - You may want to use different number of units for the two hidden layers. \n",
    "\n",
    "    - 2. Find out the optimal alpha parameter value for regularization.\n",
    "        - alpha = [0.01, 0.05, 0.1, 0.5, 1.0, 5.0]\n",
    "\n",
    "    - 3. Apply three different activation functions and show the effect.\n",
    "        - activation = ['tanh', relu', 'logistic']\n",
    "        \n",
    "    - 4. (extra) Build a NN model with 3 hidden layers and check if the model is improvled. \n",
    "\n",
    "- Compare the results without normalization and with normalization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Breast Cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast cancer dataset for classification\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
    "\n",
    "print(X_cancer.shape) # 30 attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add cells for your modeling below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first choose the first two attributes and display the data\n",
    "\n",
    "- This is simply for visualization purpose. You can include most (or all) attributes for X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should choose some attributes for X\n",
    "# You also should check X's shpae and y_cancer's shape\n",
    "# We show how to do below as an example,\n",
    "# but you should be able to do this process on your own. \n",
    "\n",
    "X = X_cancer[:, :2] \n",
    "print(X.shape)\n",
    "print(y_cancer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Sample binary classification problem with non-linearly separable classes')\n",
    "plt.scatter(X[:,0], X[:,1], c=y_cancer,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply neural network with 2 hidden layers with varying number of units (e.g., 10, 20, 50, 100) on non-normalized data\n",
    "\n",
    "- We are using all 30 attribues of X\n",
    "- X is not normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data normalization\n",
    "\n",
    "- We are using all 30 attribues of X\n",
    "- Please keep in mind that we do not normalize y values (it is class label!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply neural network with 2 hidden layers with varying number of units (e.g., 10, 20, 50, 100) on normalized data\n",
    "\n",
    "- We are using all 30 attribues of X\n",
    "- X is normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the optimal alpha parameter value for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply different activation functions (logistic, tanh, relu) and show the effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Fruit dataset\n",
    "\n",
    "- We show how to read txt format files using Pandas read_csv() function. \n",
    "- Make sure you check shape of X and shape of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fruits dataset\n",
    "fruits = pd.read_csv('./data/fruit_data_with_colors.txt', sep='\\t', engine='python')\n",
    "\n",
    "feature_names_fruits = ['height', 'width', 'mass', 'color_score']\n",
    "\n",
    "X_fruits = fruits[feature_names_fruits]\n",
    "y_fruits = fruits['fruit_label']\n",
    "\n",
    "target_names_fruits = ['apple', 'mandarin', 'orange', 'lemon']\n",
    "\n",
    "X_fruits_2d = fruits[['height', 'width']]\n",
    "y_fruits_2d = fruits['fruit_label']\n",
    "\n",
    "\n",
    "print(X_fruits_2d.shape)\n",
    "print(y_fruits_2d.shape)\n",
    "\n",
    "#print(X_fruits_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply neural network with 2 hidden layers with varying number of units (e.g., 10, 20, 50, 100) on not normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply varying regularization parameter alpha on non-normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply three different activation functions on non-normalized data¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply neural network with 2 hidden layers with varying number of units (e.g., 10, 20, 50, 100) on normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the optimal alpha parameter value for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply different activation functions (logistic, tanh, relu) and show the effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "- Write a short summary of your analysis result of neural networks (submitted on Canvas). \n",
    "\n",
    "- Provide a link to the notebook on Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
